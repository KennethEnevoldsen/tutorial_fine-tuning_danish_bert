{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bitea838f35b129458e9787f325adb7b1c3",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a Danish BERT\n",
    "\n",
    "This tutorial will take you through how to fine-tune a BERT, both for sentence and token classification. We will start of by downloading [the Danish BERT model by botXO](https://github.com/botxo/nordic_bert). You should put the content of the downloaded .zip file into the folder 췂danish_bert_uncased_v2췂, where the script 췂convert_bert_original_tf_checkpoint_to_pytorch.py췂 should already be located. If not download it from [this GitHub repository](https://github.com/KennethEnevoldsen/tutorial_fine-tuning_danish_bert).\n",
    "\n",
    "After the BERT model has been downloaded, move it to the directory and run the following line from a terminal (in said directory): \n",
    "\n",
    "```python convert_bert_original_tf_checkpoint_to_pytorch.py --tf_checkpoint_path model.ckpt --bert_config_file config.json --pytorch_dump_path pytorch_model.bin```\n",
    "\n",
    "There are several versions to choose from, but we recommend choosing the newest version. Lastly we will need to import the following packages. You might need to pip install some of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# native packages\n",
    "import os\n",
    "\n",
    "# widely use packages\n",
    "import pandas as pd\n",
    "\n",
    "# other packages\n",
    "from simpletransformers.ner import NERModel\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "from danlp.datasets import DDT\n",
    "import pyconll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Token Classification\n",
    "Let's start by doing a token classification. Token classification is the act of classifying tokens as is for example used to classify whether a token is an entity and what type of entity it is, e.g. person, organization or location. This is typically called named-entity recognition. Other token classification tasks include part-of-speech tagging as well as others. For this example we will train a BERT for named-entity recognition using the tagged data by DaNLP derived from the Danish dependency Treebank. We will start by loading in the data and examining it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Danish Dependency Tree data\n",
    "ddt = DDT()\n",
    "conllu_format = ddt.load_as_conllu(predefined_splits = True)\n",
    "\n",
    "data = []\n",
    "for n in range(len(conllu_format)):\n",
    "    data.append([(i, token.form, token.misc.get(\"name\").pop()) for i, sent in enumerate(conllu_format[n]) for token in sent]) #Getting the sentence #, Word and Tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   sentence_id      words labels\n0            0         P친      O\n1            0     fredag      O\n2            0        har      O\n3            0        SID  B-ORG\n4            0  inviteret      O\n5            0        til      O\n6            0  reception      O\n7            0          i      O\n8            0  SID-huset  B-LOC\n9            0          i      O",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>words</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>P친</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>fredag</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>har</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>SID</td>\n      <td>B-ORG</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>inviteret</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>til</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>reception</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>i</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>SID-huset</td>\n      <td>B-LOC</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>i</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# this dataset contain a training dataset\n",
    "train = pd.DataFrame(data[0], columns = ['sentence_id', 'words', 'labels']) # note that the names of the columns are important for the model\n",
    "# a development test dataset\n",
    "test = pd.DataFrame(data[1], columns = ['sentence_id', 'words', 'labels'])\n",
    "# and lastly a validation dataset\n",
    "validation = pd.DataFrame(data[2], columns = ['sentence_id', 'words', 'labels'])\n",
    "\n",
    "# examing the first ten rows we see some of the structure of the data\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so now we are ready to train the model. Beware that this process might take some time to it might be ideal to only use some of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "I0323 21:45:40.435643 4657845696 configuration_utils.py:252] loading configuration file danish_bert_uncased_v2/config.json\nI0323 21:45:40.437238 4657845696 configuration_utils.py:290] Model config BertConfig {\n  \"architectures\": null,\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"directionality\": \"bidi\",\n  \"do_sample\": false,\n  \"eos_token_ids\": 0,\n  \"finetuning_task\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"is_decoder\": false,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"length_penalty\": 1.0,\n  \"max_length\": 20,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_beams\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_labels\": 9,\n  \"num_return_sequences\": 1,\n  \"output_attentions\": false,\n  \"output_hidden_states\": false,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"pruned_heads\": {},\n  \"repetition_penalty\": 1.0,\n  \"temperature\": 1.0,\n  \"top_k\": 50,\n  \"top_p\": 1.0,\n  \"torchscript\": false,\n  \"type_vocab_size\": 2,\n  \"use_bfloat16\": false,\n  \"vocab_size\": 32000\n}\n\nI0323 21:45:40.438611 4657845696 modeling_utils.py:456] loading weights file danish_bert_uncased_v2/pytorch_model.bin\nI0323 21:45:42.971154 4657845696 modeling_utils.py:543] Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\nI0323 21:45:42.972272 4657845696 modeling_utils.py:549] Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\nI0323 21:45:42.973890 4657845696 tokenization_utils.py:335] Model name 'danish_bert_uncased_v2/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'danish_bert_uncased_v2/' is a path, a model identifier, or url to a directory containing tokenizer files.\nI0323 21:45:42.975126 4657845696 tokenization_utils.py:364] Didn't find file danish_bert_uncased_v2/added_tokens.json. We won't load it.\nI0323 21:45:42.976114 4657845696 tokenization_utils.py:364] Didn't find file danish_bert_uncased_v2/special_tokens_map.json. We won't load it.\nI0323 21:45:42.977463 4657845696 tokenization_utils.py:364] Didn't find file danish_bert_uncased_v2/tokenizer_config.json. We won't load it.\nI0323 21:45:42.978440 4657845696 tokenization_utils.py:416] loading file danish_bert_uncased_v2/vocab.txt\nI0323 21:45:42.979154 4657845696 tokenization_utils.py:416] loading file None\nI0323 21:45:42.980566 4657845696 tokenization_utils.py:416] loading file None\nI0323 21:45:42.981617 4657845696 tokenization_utils.py:416] loading file None\n"
    }
   ],
   "source": [
    "# get list of unique labels\n",
    "unique_labels = list(train['labels'].unique())\n",
    "\n",
    "# we will need to rename the config file from bert_config.json to config.json\n",
    "# os.rename('danish_bert_uncased_v2/bert_config.json', 'danish_bert_uncased_v2/config.json')\n",
    "\n",
    "# preparing the model\n",
    "model = NERModel('bert', model_name = 'danish_bert_uncased_v2/', labels=unique_labels, use_cuda=False, args={'overwrite_output_dir': True, 'reprocess_input_data': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "model.train_model(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Classification\n",
    "Sentence classification is the act of classifying a sentence. This could be classyfying the topic of a sentence or classifying whether a sentence is postive or negative. In this case we will try to predict the score of a trustpilot review based on the text of the review. The dataset used for this is avaliable in the [Github repository](https://github.com/KennethEnevoldsen/tutorial_fine-tuning_danish_bert). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                text  labels\n0  bestilte airpods der var p친 lager i f칮lge over...       0\n1                                   b칮vl  b칮vl  b칮vl       0\n2                                   d친rlig oplevelse       0\n3  bestiller en varer  som er p친 lager og vil bli...       0\n4                              modtog ikke min pakke       0\n5            der kom ikke alt hvad jeg havde bestilt       0\n6  bestilte en dvd der var markeret med  levering...       0\n7  jeg bestilte et dykkers칝t til b칮rn mellem 5 og...       0\n8  jeg fik ingen varer  p친 hjemmesiden fandt jeg ...       0\n9  virkelig d친rlig service  jeg havde bestilt en ...       0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bestilte airpods der var p친 lager i f칮lge over...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b칮vl  b칮vl  b칮vl</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>d친rlig oplevelse</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bestiller en varer  som er p친 lager og vil bli...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>modtog ikke min pakke</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>der kom ikke alt hvad jeg havde bestilt</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>bestilte en dvd der var markeret med  levering...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>jeg bestilte et dykkers칝t til b칮rn mellem 5 og...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>jeg fik ingen varer  p친 hjemmesiden fandt jeg ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>virkelig d친rlig service  jeg havde bestilt en ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "tp = pd.read_csv(\"trustpilot.csv\")\n",
    "tp.columns = ['text', 'labels'] # rename variables - not that the renames variable names are important\n",
    "\n",
    "tp['text'] = tp['text'].astype('str')\n",
    "tp['labels'] = tp['labels'] - 1 # index to zero\n",
    "tp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "I0323 22:12:02.215758 4678493632 configuration_utils.py:252] loading configuration file danish_bert_uncased_v2/config.json\nI0323 22:12:02.217087 4678493632 configuration_utils.py:290] Model config BertConfig {\n  \"architectures\": null,\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"directionality\": \"bidi\",\n  \"do_sample\": false,\n  \"eos_token_ids\": 0,\n  \"finetuning_task\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"is_decoder\": false,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"length_penalty\": 1.0,\n  \"max_length\": 20,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_beams\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_labels\": 5,\n  \"num_return_sequences\": 1,\n  \"output_attentions\": false,\n  \"output_hidden_states\": false,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"pruned_heads\": {},\n  \"repetition_penalty\": 1.0,\n  \"temperature\": 1.0,\n  \"top_k\": 50,\n  \"top_p\": 1.0,\n  \"torchscript\": false,\n  \"type_vocab_size\": 2,\n  \"use_bfloat16\": false,\n  \"vocab_size\": 32000\n}\n\nI0323 22:12:02.218364 4678493632 modeling_utils.py:456] loading weights file danish_bert_uncased_v2/pytorch_model.bin\nI0323 22:12:04.784692 4678493632 modeling_utils.py:543] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\nI0323 22:12:04.785426 4678493632 modeling_utils.py:549] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\nI0323 22:12:04.787040 4678493632 tokenization_utils.py:335] Model name 'danish_bert_uncased_v2/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'danish_bert_uncased_v2/' is a path, a model identifier, or url to a directory containing tokenizer files.\nI0323 22:12:04.787983 4678493632 tokenization_utils.py:364] Didn't find file danish_bert_uncased_v2/added_tokens.json. We won't load it.\nI0323 22:12:04.788883 4678493632 tokenization_utils.py:364] Didn't find file danish_bert_uncased_v2/special_tokens_map.json. We won't load it.\nI0323 22:12:04.789831 4678493632 tokenization_utils.py:364] Didn't find file danish_bert_uncased_v2/tokenizer_config.json. We won't load it.\nI0323 22:12:04.790503 4678493632 tokenization_utils.py:416] loading file danish_bert_uncased_v2/vocab.txt\nI0323 22:12:04.791198 4678493632 tokenization_utils.py:416] loading file None\nI0323 22:12:04.792128 4678493632 tokenization_utils.py:416] loading file None\nI0323 22:12:04.792845 4678493632 tokenization_utils.py:416] loading file None\n"
    }
   ],
   "source": [
    "# number og unique labels\n",
    "n_labels = len(tp['labels'].unique())\n",
    "\n",
    "# initialize the model\n",
    "sent_model = ClassificationModel('bert', 'danish_bert_uncased_v2/', num_labels=n_labels, use_cuda=False, args={'reprocess_input_data': True, 'overwrite_output_dir': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "ent_model.train_model(tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "That is it you have now trained two BERT models for Danish natural language processing 游봅! To use the model simply use `model.predict()`.\n",
    "\n",
    "This tutorial was made by [L. Hansen](https://github.com/HLasse), [M. Bertelsen](https://github.com/MalteHB) and [K. Enevoldsen](https://github.com/KennethEnevoldsen). Feel free to ask any question in the GitHub issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}